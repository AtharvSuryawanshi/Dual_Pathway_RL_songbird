{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate-coded version of the dual pathway model\n",
    "\n",
    "This script creates a sensorimotor landscapes by superimposing several Gaussians, and tests the dual pathway model on it.\n",
    "\n",
    "The model consists of two parallel pathways:-\n",
    "- one: performs RL and is reset every day (exploration)\n",
    "- two: maintains a record of the BG exploration (exploitation)\n",
    "\n",
    "The reset at night depends on the potentiation observed during the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxilliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate the sensorimotor landscape on which the model is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(sigma=0.1, center=None):\n",
    "        \"\"\" Randomly assigns the mean and std deviation for the hills in the reward contour. \"\"\"\n",
    "\n",
    "        if center is None:\n",
    "            # r = np.sqrt(np.random.uniform(0.0, 1.0))\n",
    "            # a = np.random.uniform(0, 2*np.pi)\n",
    "            # center = r*np.cos(a), r*np.sin(a)\n",
    "            center = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "        return center, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(n=128, center=(0,0), sigma=0.1):\n",
    "        \"\"\" Creates a 2D gaussian distribution, given the center coordinates and std deviation. \"\"\"\n",
    "\n",
    "        X, Y = np.meshgrid(np.linspace(-1, +1, n), np.linspace(-1, +1, n))\n",
    "        x0, y0 = center\n",
    "        D = np.sqrt((X-x0)**2/(2*sigma**2) + (Y-y0)**2/(2*sigma**2))\n",
    "\n",
    "        return 1/(2*np.pi*sigma**2)*np.exp(-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient_artificial(n=256, n_distractors=20):\n",
    "        \"\"\" Creates the overall reward contour by combining several gaussians. \"\"\"\n",
    "        \n",
    "        # No. of local optima\n",
    "        n_hills = n_distractors\n",
    "\n",
    "        # Target i.e. global optima\n",
    "        # target_theta = np.random.uniform(0,2*np.pi)\n",
    "        # target_r = np.random.uniform(0,1)\n",
    "        # targetpos = target_r*np.cos(target_theta), target_r*np.sin(target_theta)\n",
    "        targetpos = np.random.uniform(-1, 1, 2)\n",
    "        hills = [hill(0.3, targetpos)] # chosen sigma=0.3\n",
    "\n",
    "        # n_hills distractors (0.4 < sigma < 0.7) i.e. local optima\n",
    "        for i in range(n_hills):\n",
    "            hills.append(hill(np.random.uniform(0.4, 0.7)))\n",
    "\n",
    "        # Build gradient landscape\n",
    "        Z = np.zeros((n,n))\n",
    "        for (center, sigma) in hills:\n",
    "            Z = np.maximum(Z, gaussian(n, center, sigma))\n",
    "            # Z = Z + gaussian(n, center, sigma)    # For a smoother reward profile    \n",
    "        \n",
    "        return  Z / Z.max(), targetpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient(n=256, contour_type='Artificial'):\n",
    "        \"\"\" Redirects to the function that generates the desired contour type \"\"\"\n",
    "        \n",
    "        if contour_type == 'Syrinx': return generate_gradient_syrinx(n)\n",
    "        elif contour_type == 'Artificial': return generate_gradient_artificial(n)\n",
    "        else: print(\"Contour type not specified (Syrinx/Artificial).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gradient(Z, p):\n",
    "        \"\"\" Returns height of reward profile at given position. \"\"\"\n",
    "\n",
    "        x = min(max(p[0], -1), 1)\n",
    "        y = min(max(p[1], -1), 1)\n",
    "        col = int(((x + 1)/2) * (Z.shape[1]-1))\n",
    "        row = int(((y + 1)/2) * (Z.shape[0]-1))\n",
    "\n",
    "        return Z[row, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions.\n",
    "\n",
    "Steep sigmoidals help in making neurons function as essentially binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, m=5, a=0.5):\n",
    "    \"\"\" Returns an output between 0 and 1. \"\"\"\n",
    "    return 1 / (1 + np.exp(-1*(x-a)*m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_neg(x, m=5, a=0.5):\n",
    "    \"\"\" Returns an output between -1 and 1. \"\"\"\n",
    "    return (2 / (1 + np.exp(-1*(x-a)*m))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate of motor pathway.\n",
    "\n",
    "Vestigial code.\n",
    "Used previously to test changing learning rates in the cortical pathway over development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pPos(t):\n",
    "    \"\"\" Testing changing learning rate of HL weights \"\"\" \n",
    "\n",
    "    return 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main function to simulate learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_simulate(arg_rseed, arg_annealing=True, arg_plot=True):\n",
    "    \"\"\" Main function to build and simulate the model \"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Parameters\n",
    "    # ---------- #\n",
    "\n",
    "    # Random seed parameters\n",
    "    rSeed = int(arg_rseed)\n",
    "    np.random.seed(rSeed)\n",
    "\n",
    "    # Experiment parameterss\n",
    "    annealing = arg_annealing\n",
    "    plotting = arg_plot\n",
    "\n",
    "    # Output parameters\n",
    "    resname = str(rSeed)\n",
    "    if annealing == True: resname = resname + '_annealing'\n",
    "\n",
    "    print(resname)\n",
    "\n",
    "    # Layer parameters\n",
    "    HVC_total_size = 100              \n",
    "    BG_size = 50\n",
    "    # RA_inh_size = 2\n",
    "    RA_size = 100\n",
    "    MC_size = 2\n",
    "    n_BG_clusters = 10\n",
    "    n_RA_clusters = MC_size\n",
    "\n",
    "    # Weight parameters\n",
    "    soft_bound = False                          \n",
    "    Wmin_f, Wmax_f = 0, 1\n",
    "    Wmin_RL, Wmax_RL = -1, 1\n",
    "    Wmin_HL, Wmax_HL = -1, 1\n",
    "    Wepsilon = 0.1\n",
    "\n",
    "    # Input encoding\n",
    "    n_syllables = 1                                            # setting the no. of syllables to simulate; keep in mind the maximum syllables possible without overlap in encoding\n",
    "    n_bits = HVC_total_size//10                                # no. of bits active in each syllable encoding; corresponds with 10% of HVC being active for one timepoint in song\n",
    "    HVC_size = n_bits * n_syllables                            # effective size of HVC\n",
    "\n",
    "\n",
    "    # Simulation parameters\n",
    "    n_daily_motifs = 1000\n",
    "    n_days = 60\n",
    "    n_trial_per_syll = n_daily_motifs * n_days \n",
    "    n_intact_trials = n_trial_per_syll * n_syllables            # Both pathways are active in intact trials\n",
    "    n_lesioned_days = 1                                         # to check Hebbian learning result without BG input\n",
    "    n_total_trials = n_intact_trials + n_lesioned_days * n_daily_motifs * n_syllables\n",
    "\n",
    "    # Layer interactions\n",
    "    RA_inter_neurons = False                                             # To include inhibitory inter-neurons in RA\n",
    "    Hebbian_learning = HEBBIAN_LEARNING = True\n",
    "    BG_influence = True\n",
    "    Hebbian_rule = 1                                                     # Type of learning rule: 1 -> Hebbian, 2 -> iBCM, 3 -> Oja\n",
    "    eta = ETA = 0.1\n",
    "\n",
    "    # Neuron parameters\n",
    "    BG_noise_mean = 0.00                                                 # Activation function parameters\n",
    "    BG_noise_std = 0.05\n",
    "    RA_noise_mean = 0.00\n",
    "    RA_noise_std = 0.01\n",
    "\n",
    "    BG_sig_slope = 10                                                    # BG sigmoidal should be as less steep as possible\n",
    "    BG_sig_mid = 0\n",
    "    RA_sig_slope = 30                                                    # RA sigmoidal should be as steep as possible\n",
    "    RA_sig_mid = 0\n",
    "\n",
    "    # Reward computation parameters: performance is computed with a window over recent trials\n",
    "    reward_window = 25                      \n",
    "    reward_sigma = 0.2\n",
    "\n",
    "\n",
    "    # Model build\n",
    "    # ----------- #\n",
    "    HVC = np.zeros(HVC_size)\n",
    "    RA = np.zeros(RA_size)\n",
    "    MC = np.zeros(MC_size)\n",
    "    BG = np.zeros(BG_size)\n",
    "\n",
    "    # Randomised initialisation\n",
    "    W_HVC_RA = np.zeros((HVC_size, RA_size), float) #+ .5 # Wepsilon                                        # HVC-RA connections should start from non-existent;\n",
    "    W_HVC_BG = np.random.uniform(Wmin_RL + Wepsilon, Wmax_RL - Wepsilon, (HVC_size, BG_size))               # HVC-BG randomly initialised\n",
    "    W_BG_RA = np.random.uniform(Wmin_f + Wepsilon, Wmax_f - Wepsilon, (BG_size, RA_size))                   # BG-RA generated randomly and remains fixed over the simulation\n",
    "    W_RA_MC = np.random.uniform(Wmin_f + Wepsilon, Wmax_f - Wepsilon, (RA_size, MC_size))                   # RA-MC generated randomly and remains fixed over the simulation\n",
    "\n",
    "    # Segregated pathways between RA and MC (topological connections)\n",
    "    RA_cluster_size = RA_size//n_RA_clusters\n",
    "    BG_cluster_size = BG_size//n_BG_clusters\n",
    "    RA_channel_size = RA_size//n_BG_clusters\n",
    "\n",
    "    for i in range(n_RA_clusters):\n",
    "        segPath = np.diag(np.ones(n_RA_clusters, int))[i]\n",
    "        W_RA_MC[i*RA_cluster_size : (i+1)*RA_cluster_size] *= segPath\n",
    "\n",
    "    # Segregated pathways between BG and RA\n",
    "    for i in range(n_BG_clusters):\n",
    "        segPath = np.diag(np.ones(n_BG_clusters, int))[i]\n",
    "        W_BG_RA[i*BG_cluster_size : (i+1)*BG_cluster_size] *= [j for j in segPath for r in range(RA_channel_size)]\n",
    "\n",
    "    # Sparsity increases as number of connections/neurons increases\n",
    "    # idx1 = np.random.choice(np.arange(W_HVC_BG.size), size=int(np.sqrt(W_HVC_BG.size)), replace=False)\n",
    "    # W_HVC_BG[idx1//BG_size, idx1%BG_size] = 0\n",
    "    idx = np.random.choice(np.arange(W_BG_RA.size), size=int(np.sqrt(W_BG_RA.size)), replace=False)\n",
    "    W_BG_RA[idx//RA_size, idx%RA_size] = 0\n",
    "    idx = np.random.choice(np.arange(W_RA_MC.size), size=int(np.sqrt(W_RA_MC.size)), replace=False)\n",
    "    W_RA_MC[idx//MC_size, idx%MC_size] = 0\n",
    "\n",
    "\n",
    "    ## Syllable encoding (input)\n",
    "    syllable_encoding = {}\n",
    "    syllables = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]              # General repertoire\n",
    "    syllables = syllables[:n_syllables]                                         # ensure n_samples >= len(syllables)\n",
    "\n",
    "    # Eg. 3 bit encoding for 2 syllables in a 10 neuron HVC:- \"A\" 1110000000, \"B\" 0001110000\n",
    "    for i in range(n_syllables):\n",
    "        inputs = np.zeros(HVC_size)\n",
    "        inputs[i * n_bits:(i + 1) * n_bits] = 1\n",
    "        syllable_encoding[syllables[i]] = inputs\n",
    "\n",
    "    # Different reward landscape for each syllable\n",
    "    syllable_targets = {}\n",
    "    syllable_contours = {}\n",
    "    for i in range(n_syllables):\n",
    "        contour, target = generate_gradient()\n",
    "        syllable_contours[syllables[i]] = contour\n",
    "        syllable_targets[syllables[i]] = target\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- #\n",
    "\n",
    "    # Tracking\n",
    "    R = np.zeros((n_syllables, n_total_trials//n_syllables))                                # keeps track of reward\n",
    "    W_RL = np.zeros((n_total_trials, n_bits*BG_size))                                       # tracks HVC-BG weights\n",
    "    W_HL = np.zeros((n_total_trials, W_HVC_RA.size))                                        # tracks HVC-RA weights\n",
    "\n",
    "    S_MO = np.zeros((n_total_trials, MC_size))                                              # tracks the 2D output\n",
    "    S_T = np.zeros((n_total_trials, MC_size))                                               # tracks target\n",
    "    \n",
    "    S_RA = np.zeros((n_total_trials, RA_size))                                              # tracks RA activity\n",
    "    S_BG = np.zeros((n_total_trials, BG_size))                                              # tracks BG activity\n",
    "\n",
    "    S_dW1_sum = np.zeros((n_syllables, n_total_trials//n_syllables))                        # tracks sum of change in BG weights over a given day (proxy for measure of potentiation over the day)\n",
    "    S_p = np.zeros((n_total_trials))                                                        # function of S_dW1_sum\n",
    "\n",
    "    S_RA_HVC = np.zeros((n_total_trials, RA_size))                                          # tracks input to each RA neuron from the HVC\n",
    "    S_RA_BG = np.zeros((n_total_trials, RA_size))                                           # tracks input to each RA neuron from the BG\n",
    "\n",
    "    sum_W_HVC_BG = np.sum((W_HVC_BG!=0), axis=0)                                            # Used for normalisation\n",
    "    sum_W_BG_RA = np.sum(np.abs(W_BG_RA), axis=0)                                           # Used for normalisation\n",
    "    sum_W_RA_MC = np.sum(W_RA_MC, axis=0)                                                   # Used for normalisation\n",
    "\n",
    "\n",
    "    RA_HVC = np.zeros((RA_size))                                                            # current input to each RA neuron from the HVC\n",
    "    RA_BG = np.zeros((RA_size))                                                             # current input to each RA neuron from the BG\n",
    "    dW_RL = np.zeros(W_HVC_BG.shape)                                                        # current change to RL weights\n",
    "    dW_HL = np.zeros(W_HVC_RA.shape)                                                        # current change to HL weights\n",
    "    night_noise = np.zeros(W_HVC_BG.shape)                                                  # nightly noise induced within RL weights\n",
    "    dW_night = np.zeros(W_HVC_BG.shape)                                                     # nightly displacement of RL weights\n",
    "\n",
    "\n",
    "    #### Main simulation of learning\n",
    "    nt = 0\n",
    "    inh_lesion = 1\n",
    "    \n",
    "    # Simulate over a some training days + one day with BG lesioned off\n",
    "    for n_day in np.arange(n_days + n_lesioned_days):\n",
    "        day_dW1_sum = np.zeros((n_syllables))                                               # Tracks BG weights potentiation over the day\n",
    "        day_R_sum = np.zeros((n_syllables))                                                 # Tracks reward obtained over the day\n",
    "        \n",
    "        # Turn BG off if training is over\n",
    "        if BG_influence==1 and n_day >= n_days:\n",
    "            BG_influence = 0\n",
    "            \n",
    "        # Simulate a given number of motifs each day\n",
    "        for n_motif in np.arange(n_daily_motifs):\n",
    "             \n",
    "            # (1 motif = 1 iteration over all syllables)\n",
    "            for n_syll in np.arange(n_syllables):\n",
    "                # Tracking iteration number wrt motifs and syllables\n",
    "                n_iter = int(n_day * n_daily_motifs + n_motif)\n",
    "                nt = n_iter * n_syllables + n_syll\n",
    "\n",
    "                # Syllable input and target encoding\n",
    "                syll = syllables[n_syll]                                                    \n",
    "                target = syllable_targets[syll]\n",
    "                contour = syllable_contours[syll]\n",
    "                HVC[...] = syllable_encoding[syll]\n",
    "\n",
    "                \n",
    "                # Compute BG\n",
    "                BG[...] = np.dot(HVC, W_HVC_BG) / sum_W_HVC_BG + np.random.normal(BG_noise_mean, BG_noise_std, BG_size)\n",
    "                BG[...] = sigmoid_neg(BG, BG_sig_slope, BG_sig_mid)\n",
    "            \n",
    "\n",
    "                # Compute RA activity\n",
    "                RA_HVC[...] = np.dot(HVC, W_HVC_RA) / n_bits * Hebbian_learning\n",
    "                RA_BG[...] = np.dot(BG, W_BG_RA) / sum_W_BG_RA * BG_influence * 2\n",
    "        \n",
    "                RA =  ((RA_HVC) * Hebbian_learning) + (RA_BG * BG_influence)\n",
    "                \n",
    "                RA[...] = sigmoid_neg(RA, RA_sig_slope, RA_sig_mid)\n",
    "\n",
    "                RA_noise = np.random.normal(RA_noise_mean, RA_noise_std, RA_size)                                       # Currently this has been set to extremely low - so for now, it is ineffective\n",
    "                RA = RA + RA_noise\n",
    "\n",
    "                # Compute MC activity\n",
    "                MC[...] = (np.dot(RA, W_RA_MC)/sum_W_RA_MC)\n",
    "\n",
    "                # Print 2D output at first trial (initial point)\n",
    "                if nt == 0: print(MC)                                       \n",
    "                \n",
    "\n",
    "                # Compute error and reward\n",
    "                reward = read_gradient(contour, MC)\n",
    "\n",
    "\n",
    "                # Computing mean recent reward\n",
    "                R_ = 0\n",
    "                if n_iter > reward_window: R_ = R[n_syll, n_iter - reward_window:n_iter].mean()\n",
    "                elif n_iter > 0: R_ = R[n_syll, :n_iter].mean()\n",
    "\n",
    "                # Weight update\n",
    "                dW_RL[...] = eta * (reward - R_) * HVC.reshape(HVC_size, 1) * (BG) * BG_influence\n",
    "\n",
    "\n",
    "                # Tau for lBCM (Law and Cooper, 1994)\n",
    "                theta_M = 1\n",
    "                \n",
    "                \n",
    "                if Hebbian_rule == 1:\n",
    "                    dW_HL[...] = pPos(n_iter) * HVC.reshape(HVC_size, 1) * (RA) * Hebbian_learning\n",
    "\n",
    "                elif Hebbian_rule == 2:\n",
    "                    if nt > 500:    theta_M = np.power(np.mean(S_RA[nt-500:nt], axis=0)-.5, 2)\n",
    "                    elif nt > 0:    theta_M = np.power(np.mean(S_RA[:nt], axis=0)-.5, 2)\n",
    "                    dW_HL[...] = pPos(n_iter) * HVC.reshape(HVC_size, 1) * (RA) / (theta_M+Wepsilon) * (RA-theta_M) * Hebbian_learning                                                                  # iBCM\n",
    "\n",
    "                elif Hebbian_rule == 3:\n",
    "                    dW_HL[...] = pPos(n_iter) * ((HVC.reshape(HVC_size, 1) * RA) - (HVC.reshape(HVC_size, 1) * RA * RA * W_HVC_RA)) * Hebbian_learning                                                 # Oja learning rule                                                                  RA_size) * Hebbian_learning\n",
    "\n",
    "                # Track potentiation received\n",
    "                day_dW1_sum[n_syll] += np.mean((np.abs(dW_RL))[n_bits*(n_syll):n_bits*(n_syll+1)])\n",
    "\n",
    "                # Bounding weights (asymptotically/abruptly)\n",
    "                if soft_bound == 1:\n",
    "                    # W_HVC_BG += dW_RL * (Wmax_RL - W_HVC_BG) * (W_HVC_BG - Wmin_RL)\n",
    "                    W_HVC_RA[...] += dW_HL * (Wmax_HL - W_HVC_RA) * (W_HVC_RA - Wmin_HL)\n",
    "                else:\n",
    "                    W_HVC_RA[...] = np.minimum(Wmax_HL, np.maximum(Wmin_HL, W_HVC_RA + dW_HL))\n",
    "                    W_HVC_BG[...] = np.minimum(Wmax_RL, np.maximum(Wmin_RL, W_HVC_BG + dW_RL))\n",
    "                \n",
    "\n",
    "                # For tracking purposes\n",
    "                R[n_syll, n_iter] = reward\n",
    "                W_RL[nt] = W_HVC_BG[:n_bits,:].ravel()                                \n",
    "                W_HL[nt] = W_HVC_RA.ravel()\n",
    "                S_MO[nt] = MC.ravel()\n",
    "                S_T[nt] = target\n",
    "                S_RA[nt] = RA\n",
    "                S_RA_HVC[nt] = RA_HVC\n",
    "                S_RA_BG[nt] = RA_BG\n",
    "                S_BG[nt] = BG\n",
    "                S_dW1_sum[n_syll, n_iter] = day_dW1_sum[n_syll]\n",
    "        \n",
    "\n",
    "        ## Simulating nightly jumps within BG pathway\n",
    "        if annealing == True:\n",
    "            # Night shuffling of HVC-BG synapses\n",
    "            p = day_dW1_sum[n_syll]/10                                                                      # p is proportional to daily sum of change in BG weights\n",
    "            p = sigmoid(p, 10)\n",
    "\n",
    "            # When potentiation is high during a day, we simulate a low disturbance in the night \n",
    "            potentiation_factor = np.zeros((HVC_size))\n",
    "            potentiation_factor[:(n_bits*n_syllables)] = 1-p                                                # potentiation_factor is inversely proportional to p\n",
    "\n",
    "            S_p[nt] = 1-p \n",
    "\n",
    "            night_noise[...] = np.random.uniform(-1, 1, (BG_size)) * BG_noise_std * 1000\n",
    "            dW_night[...] = eta * (potentiation_factor.reshape(HVC_size,1)) * (night_noise) * BG_influence\n",
    "            print(np.max(dW_night))\n",
    "            W_HVC_BG[...] = (W_HVC_BG + dW_night + 1)%2-1                                                   # modulus so that the high noise doesn't make the weights to saturate\n",
    "\n",
    "\n",
    "    if plotting == False: # change it to true\n",
    "        # Save results for plotting\n",
    "        np.savetxt('Results/'+resname+'_endRA.txt', S_RA[-1]>.5)\n",
    "\n",
    "        test_k = 10                                                                                         # Chooses which particular RA neuron to observe\n",
    "\n",
    "        # # Plot inputs to RA_exc\n",
    "        # plt.close()\n",
    "\n",
    "        # plt.plot(S_RA_BG[:,test_k], label='BG',lw=0.5, marker=',', alpha=.5)\n",
    "        # # plt.plot(S_RA_inh[:,test_k], label='inh',lw=0.5, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA_BG[:,test_k]+S_RA_HVC[:,test_k], label='BG+HVC',lw=0.5, marker=',', alpha=.5)\n",
    "        # # plt.plot(S_RA_BG[:,test_k]+S_RA_HVC[:,test_k]-S_RA_exc_inh[:,test_k], label='BG+HVC-inh',lw=0.5, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA[:,test_k], label='sig(input)',lw=0, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA_HVC[:,test_k], label='HVC',lw=0.5, marker=',')\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_RA_inputs.png')\n",
    "        # plt.close()\n",
    "        \n",
    "\n",
    "        # Plot sum of dW1\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # plt.plot(S_p, lw=0, marker='.', label='pf')\n",
    "        # plt.plot(S_dW1_sum.flatten(), lw=0.2, marker=',', label='daily sum')\n",
    "        # plt.plot(R.flatten(), lw=0.2, marker=',', alpha=.1, label='R')\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_dW1_sum.png')\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "        # # Plot RA_exc vs BG\n",
    "        # fig = plt.figure()\n",
    "        # plt.hist(S_RA.ravel()[::100], label='RA', alpha=.5);\n",
    "        # plt.hist(S_BG.ravel()[::100], label='BG', alpha=.5);\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_RA.png')\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # Display overall results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.rc(\"ytick\", labelsize=\"small\")\n",
    "        plt.rc(\"xtick\", labelsize=\"small\")\n",
    "\n",
    "        n_subplots = 6\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 1)\n",
    "        ax.set_title(\"Testing RL with normalised sigmoid\")\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # Plot all trial rewards\n",
    "        T = np.arange(len(R.T.flatten()))\n",
    "        ax.plot(T[::10], R.T.flatten()[::10], marker=\"o\", markersize=1.5, linewidth=0, alpha=.25,\n",
    "                color=\"none\", markeredgecolor=\"none\", markerfacecolor=\"black\")\n",
    "        ax.set_xlabel(\"Trial #\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 2)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, W_RL[:,test_k:5000:1000], color='black', alpha=.5, linewidth=0.5)\n",
    "        ax.set_ylabel(\"HVC - BG weights\")\n",
    "        ax.set_ylim(Wmin_RL-0.1, Wmax_RL+0.1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 3)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_BG[:,::200], color='black', alpha=.5, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.set_ylabel(\"BG output\")\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 4)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, W_HL[:,test_k:5000:1000], color='black', alpha=.5, linewidth=0.5)\n",
    "        ax.set_ylabel(\"HVC - RA weights\")\n",
    "        ax.set_ylim(Wmin_HL-0.1, Wmax_HL+0.1) \n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 5)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_MO, color='black', alpha=.5, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.plot(T, S_T, color='red', marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.set_xlabel(\"Trials #\")\n",
    "        ax.set_ylabel(\"Output\")\n",
    "        ax.set_ylim(-1, 1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 6)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_RA[:,test_k], color='black', alpha=.1, marker=',', linewidth=0, markersize=0.5)\n",
    "        # ax.plot(T, S_RA_inh, color='red', alpha=.1, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.axhline(y=0, color='red', alpha=.5, marker=',', linewidth=0.5)\n",
    "        ax.axhline(y=0.5, color='black', alpha=.5, marker=',', linewidth=0.5)\n",
    "        ax.set_ylabel(\"RA activity\")\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Results/' + resname + '_overall.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Plot trajectory on 2d plane\n",
    "        \"\"\" Plots the trajectory of the model output and cortical pathway over the simulation. \"\"\"\n",
    "\n",
    "        for n_syll in np.arange(n_syllables):\n",
    "            syll = syllables[n_syll]\n",
    "            contour = syllable_contours[syll]\n",
    "            target = syllable_targets[syll]\n",
    "\n",
    "\n",
    "            figure = plt.figure(figsize=(8,8))\n",
    "            ax = plt.subplot(frameon=True)\n",
    "\n",
    "            sk = n_syllables\n",
    "\n",
    "            # Plot reward contour as circles around hills    \n",
    "            contour_plot = ax.contourf(contour, 30, extent=[-1,1,-1,1], cmap=\"gray_r\", alpha=.25)\n",
    "            contour_plot = ax.contour(contour, 10, extent=[-1,1,-1,1], colors=\"black\",  alpha=.5, linewidths=0.5)\n",
    "\n",
    "            # Plot reward contour as a color plot\n",
    "            im = ax.imshow(contour, vmin=0, vmax=np.max(contour), cmap='Purples', extent=[-1,1,-1,1], origin='lower')\n",
    "\n",
    "            # Plot trajectory\n",
    "            list_colors = cm.plasma(np.linspace(0, 1, 5))\n",
    "            for day_index in np.arange(n_days+n_lesioned_days):\n",
    "                ax.plot(S_MO[n_syll+day_index*n_daily_motifs:n_syll+(day_index+1)*n_daily_motifs:sk,0],S_MO[n_syll+day_index*n_daily_motifs:n_syll+(day_index+1)*n_daily_motifs:sk,1], color=list_colors[day_index%5], lw=0, marker='.', alpha=0.1, markersize=2)\n",
    "            \n",
    "            # # Annotations\n",
    "            ax.scatter(S_MO[n_syll,0],S_MO[n_syll,1], color=\"sienna\", marker=\"o\",\n",
    "                    linewidths=3, facecolors=\"sienna\", zorder=10, label='Initial point')\n",
    "            ax.scatter(S_MO[-n_syllables+n_syll,0],S_MO[-n_syllables+n_syll,1], s=100, color=\"orange\", marker=\"x\",\n",
    "                    linewidths=3, facecolors=\"orange\", zorder=10, label='Final point')\n",
    "\n",
    "            # Display colorbar\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.5)\n",
    "            cbar = figure.colorbar(im, cax=cax, ticks=[0, .5, 1])\n",
    "            cbar.set_label('Performance metric (R)', rotation=270, fontsize=25, labelpad=25)\n",
    "            cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "            ax.set_xlabel(r'$P_{\\beta}$', fontsize=30)\n",
    "            ax.set_ylabel(r'$P_{\\alpha}$', fontsize=30)\n",
    "            ax.set_xticks(np.linspace(-1, 1, 3))\n",
    "            ax.set_yticks(np.linspace(-1, 1, 3))\n",
    "            ax.tick_params(labelsize=25)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.savefig('Results/' + resname + '_trajectory.png')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Returns terminal performance (avg performance over last training day, and avg performance on lesion day\n",
    "    return(np.mean(R[0,60000-1000:60000-1]), np.mean(R[0,-1000:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running multiple simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#### Sample simulation of learning with and without annealing for a given rseed (with descriptive plots)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mbuild_and_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m491263\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m build_and_simulate(\u001b[38;5;241m491263\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mbuild_and_simulate\u001b[1;34m(arg_rseed, arg_annealing, arg_plot)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_and_simulate\u001b[39m(arg_rseed, arg_annealing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, arg_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\" Main function to build and simulate the model \"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Parameters\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# ---------- #\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Random seed parameters\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     rSeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(arg_rseed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "#### Sample simulation of learning with and without annealing for a given rseed (with descriptive plots)\n",
    "\n",
    "build_and_simulate(491263, True, True)\n",
    "build_and_simulate(491263, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall seed:  0\n"
     ]
    }
   ],
   "source": [
    "#### Multiple simulation of learning with and without annealing (with or without descriptive plots)\n",
    "\n",
    "# Master seed to control the starting seeds generated for each simulation in the test batch\n",
    "overall_seed = 0\n",
    "np.random.seed(overall_seed)\n",
    "print('Overall seed: ', overall_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating starting seeds for each simulation in the test batch\n",
    "\n",
    "n_simulations = 10\n",
    "seeds = np.random.randint(0, 1e7, n_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording terminal performance of each simulation\n",
    "\n",
    "performance_annealing = np.zeros(n_simulations)\n",
    "performance_no_annealing = np.zeros(n_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8325804_annealing\n",
      "[ 0.13866744 -0.03879846]\n",
      "4.735062063968962\n",
      "4.457497751291981\n",
      "4.57443464779038\n",
      "3.305241780940749\n",
      "3.909071333354904\n",
      "3.4069178990346636\n",
      "4.516265743849683\n",
      "3.018597387601245\n",
      "3.138127494559857\n",
      "3.538354156032709\n",
      "4.510167599829307\n",
      "2.571437254861359\n",
      "4.4589387984949935\n",
      "4.2671291475149316\n",
      "3.9945586037711758\n",
      "3.8077596621046292\n",
      "3.4017907661207665\n",
      "3.4718310360963898\n",
      "4.085619604435101\n",
      "3.9443512775287353\n",
      "4.022586631549136\n",
      "4.350853279149407\n",
      "3.2952946287935525\n",
      "4.589057760269122\n",
      "4.324247560252708\n",
      "3.8123815458781376\n",
      "4.119642847632705\n",
      "4.577312239422607\n",
      "4.629258940992512\n",
      "4.071165717203823\n",
      "4.574346315471708\n",
      "4.65879385436242\n",
      "4.588797562861973\n",
      "4.069173307785134\n",
      "4.51187000250781\n",
      "4.218301342829765\n",
      "4.627916051787283\n",
      "4.639236919783418\n",
      "4.660737500668392\n",
      "4.050637243557865\n",
      "4.676031766139143\n",
      "4.634337830463532\n",
      "4.547058387451077\n",
      "4.536806918025076\n",
      "3.832361040899289\n",
      "4.315014121217622\n",
      "4.104164234288224\n",
      "4.514544397908663\n",
      "4.491700371478405\n",
      "4.2331297844842055\n",
      "4.699834322161371\n",
      "4.715540389596366\n",
      "4.7391667763903005\n",
      "4.803043663819853\n",
      "4.676862139371472\n",
      "4.523826318018853\n",
      "4.082092795866755\n",
      "4.538682252922221\n",
      "4.542659381375684\n",
      "4.351862923648679\n",
      "0.0\n",
      "8325804\n",
      "[ 0.13866744 -0.03879846]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:28<04:15, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484405_annealing\n",
      "[ 0.49806912 -0.31132766]\n",
      "4.324878111966856\n",
      "3.725479045024026\n",
      "3.9387028477442954\n",
      "3.766076066903968\n",
      "4.154273974634873\n",
      "4.394682722379847\n",
      "3.751205921251962\n",
      "3.5182722348710165\n",
      "4.378458807495182\n",
      "3.949841513986165\n",
      "3.1315307688435463\n",
      "4.206399907374396\n",
      "3.9670102080490848\n",
      "3.941892871554083\n",
      "4.099948506255076\n",
      "4.531679112601361\n",
      "2.520362512790468\n",
      "4.662425145296526\n",
      "4.18026970769684\n",
      "4.15762802749003\n",
      "4.418640763589298\n",
      "3.7484801672274273\n",
      "3.3451968615751233\n",
      "4.8142466627548375\n",
      "3.043952906256059\n",
      "3.9133086779793333\n",
      "3.619603378586207\n",
      "4.398633262386063\n",
      "4.200063122869203\n",
      "3.9366311108012475\n",
      "4.169808946860593\n",
      "4.18927107960839\n",
      "4.379352358853628\n",
      "4.366283551455504\n",
      "3.9460562058390303\n",
      "4.082739468685214\n",
      "4.679855523391912\n",
      "3.813781029654383\n",
      "3.5644546638104173\n",
      "4.374009956529532\n",
      "4.240058372477997\n",
      "4.353050949603913\n",
      "4.731375057114863\n",
      "4.312636336001611\n",
      "4.464997491140895\n",
      "4.5842834663073795\n",
      "4.310220667742253\n",
      "4.817989720169603\n",
      "4.35409065864211\n",
      "3.1898530598863357\n",
      "4.297696169468718\n",
      "4.043328106377721\n",
      "4.255059004519174\n",
      "4.516124287897825\n",
      "4.193178436712937\n",
      "4.406059529157966\n",
      "4.680310324504219\n",
      "4.459597453317239\n",
      "4.674391768189073\n",
      "4.695422406720946\n",
      "0.0\n",
      "1484405\n",
      "[ 0.49806912 -0.31132766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:59<03:59, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2215104_annealing\n",
      "[-0.16773095  0.33087046]\n",
      "4.729564152712009\n",
      "3.9840167548166163\n",
      "3.6045593965326104\n",
      "3.5436684961418345\n",
      "3.9756029889730393\n",
      "2.721662488892504\n",
      "1.9386320919566602\n",
      "2.9505381952022183\n",
      "4.082706950400634\n",
      "4.283467211849273\n",
      "3.712991242632783\n",
      "4.193095480943462\n",
      "4.134913396767371\n",
      "3.245404014443573\n",
      "3.480691269896209\n",
      "4.021967715425962\n",
      "4.348556016467026\n",
      "3.3144669314717263\n",
      "3.9607295983118758\n",
      "2.866703660700191\n",
      "4.217925908615368\n",
      "3.8432589477428687\n",
      "3.4362894213263875\n",
      "3.483490479553494\n",
      "4.264955354509234\n",
      "3.8680288028218963\n",
      "4.470368446051411\n",
      "4.129768214111859\n",
      "4.452502279149793\n",
      "4.335057085802318\n",
      "4.250854728941269\n",
      "4.529808890625253\n",
      "4.478242256745655\n",
      "3.8594202267289814\n",
      "4.550788764929552\n",
      "4.328936932421265\n",
      "4.148793096134423\n",
      "4.399816132114099\n",
      "3.9967946118953006\n",
      "4.370983537421228\n",
      "3.8707331730042664\n",
      "4.0064307392718765\n",
      "4.55160651792317\n",
      "3.8330751832831127\n",
      "4.187397468080607\n",
      "4.540789910754364\n",
      "4.202084422736802\n",
      "3.93547583464718\n",
      "4.350304036144101\n",
      "4.507217670213187\n",
      "4.290468623589375\n",
      "4.756488509611457\n",
      "3.6169856553657405\n",
      "3.863489179222137\n",
      "4.617335291587088\n",
      "4.422415387918397\n",
      "4.554630802272064\n",
      "4.478978182383155\n",
      "4.435004211543067\n",
      "4.452693365749981\n",
      "-0.0\n",
      "2215104\n",
      "[-0.16773095  0.33087046]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:28<03:26, 29.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5157699_annealing\n",
      "[-0.46821022  0.31365896]\n",
      "4.062431953677527\n",
      "3.6225306043275594\n",
      "2.935841977817985\n",
      "2.6356957744723664\n",
      "3.473896097728592\n",
      "3.855760179558198\n",
      "4.25977832088765\n",
      "4.733059551100396\n",
      "4.261599513733606\n",
      "4.15445338908886\n",
      "3.7852179237519277\n",
      "3.7411173671166793\n",
      "4.0390087743324665\n",
      "3.9958654667748075\n",
      "4.699009841597861\n",
      "3.7387101684043302\n",
      "3.039294123976578\n",
      "2.836076970707371\n",
      "4.5674002932888005\n",
      "2.893610791012124\n",
      "4.704246824680119\n",
      "3.723952221926104\n",
      "3.9621503054367806\n",
      "4.2074341225967\n",
      "4.510685730767047\n",
      "2.977174916503117\n",
      "3.9398122119342562\n",
      "4.2876543810769565\n",
      "3.0824477165882986\n",
      "3.6543345255697997\n",
      "3.9400222327012364\n",
      "3.2142472516858493\n",
      "4.213985677753547\n",
      "4.0674077869225185\n",
      "3.9629617232438217\n",
      "4.6230673849709\n",
      "4.022994439599252\n",
      "4.087897631588606\n",
      "4.662754999938765\n",
      "4.380559529704893\n",
      "4.056659016759813\n",
      "3.7920942991179594\n",
      "4.503119916848981\n",
      "4.2899729753101985\n",
      "4.536664522776049\n",
      "4.3834957350427\n",
      "3.638972751207995\n",
      "3.7101764921635954\n",
      "4.488845466166895\n",
      "4.3589307627000595\n",
      "4.569486336208775\n",
      "3.7731136993126033\n",
      "4.603228181534754\n",
      "4.041957457020374\n",
      "4.119579657839371\n",
      "4.508867865469483\n",
      "4.802879132987799\n",
      "4.608317392697345\n",
      "4.586176011189255\n",
      "4.6552719734067365\n",
      "0.0\n",
      "5157699\n",
      "[-0.46821022  0.31365896]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:56<02:53, 28.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222403_annealing\n",
      "[ 0.29002286 -0.08836048]\n",
      "4.441618010674472\n",
      "3.9982633813101347\n",
      "3.4179102480484147\n",
      "4.308267458285151\n",
      "3.8852536302134877\n",
      "1.9977137840196038\n",
      "4.792980967971191\n",
      "3.630440341153437\n",
      "3.219246061709161\n",
      "4.208070861046871\n",
      "4.480250098314336\n",
      "3.9306707545983803\n",
      "3.589653394887414\n",
      "3.602138745296182\n",
      "3.836822549923532\n",
      "4.338501680564204\n",
      "3.6263347520366045\n",
      "4.587892569491294\n",
      "4.18031251813029\n",
      "4.414882761839647\n",
      "3.802346449211189\n",
      "3.8114427923827927\n",
      "4.090082697677236\n",
      "3.8214444545249395\n",
      "4.076004201037687\n",
      "4.748652208055248\n",
      "4.34327912972794\n",
      "4.651260456217949\n",
      "4.609418823985892\n",
      "4.629756944010171\n",
      "4.293885502024096\n",
      "4.3091962364471526\n",
      "4.389596409252373\n",
      "3.3303056029878344\n",
      "3.9857891978616795\n",
      "4.664293472277596\n",
      "4.556609601315194\n",
      "4.457919316226037\n",
      "4.248747760877169\n",
      "4.637473923436116\n",
      "4.64144182445386\n",
      "4.678640178185364\n",
      "4.228247555183429\n",
      "4.494153741408027\n",
      "4.358645891028962\n",
      "4.541180792569765\n",
      "4.605592693826396\n",
      "4.318397011548134\n",
      "4.286291214045023\n",
      "4.496633010082304\n",
      "4.694528195853393\n",
      "4.439576487143532\n",
      "4.602953380629615\n",
      "4.342052667506022\n",
      "4.439541280723596\n",
      "4.204961332222464\n",
      "4.5959008051825645\n",
      "4.518441565027873\n",
      "4.4088300297386125\n",
      "4.250241417085297\n",
      "-0.0\n",
      "8222403\n",
      "[ 0.29002286 -0.08836048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:28<02:31, 30.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7644169_annealing\n",
      "[-0.59377638 -0.05881897]\n",
      "4.684399343735093\n",
      "4.478156409918069\n",
      "4.573805680334227\n",
      "3.9122517370305006\n",
      "4.617135386923987\n",
      "3.9225228315635006\n",
      "3.649893224725208\n",
      "4.6015594156823205\n",
      "4.083115586778645\n",
      "4.483633715011999\n",
      "4.400014187711406\n",
      "4.472879119486484\n",
      "3.650841537943524\n",
      "4.069042530624935\n",
      "4.3795215891883865\n",
      "2.8642368228460624\n",
      "4.166144737885083\n",
      "4.507206287419087\n",
      "4.177617005920149\n",
      "3.4904435636489928\n",
      "3.7190374832979365\n",
      "3.791759258957294\n",
      "4.465843747971236\n",
      "4.15030229300603\n",
      "4.278181286695273\n",
      "4.076409504075476\n",
      "4.157628440722576\n",
      "4.478576924358502\n",
      "3.994475287362417\n",
      "4.039986248789067\n",
      "4.409133144005216\n",
      "4.447564203405356\n",
      "4.063787156090654\n",
      "4.774869547680022\n",
      "4.474608085588935\n",
      "4.742842004534306\n",
      "4.453729458353357\n",
      "4.724666108736094\n",
      "4.215153025458429\n",
      "3.905510775225792\n",
      "4.323606014926056\n",
      "3.7683565103478904\n",
      "4.339083920944191\n",
      "3.9435642848277777\n",
      "4.443744281331115\n",
      "4.458410070116018\n",
      "4.484368993178664\n",
      "4.319902320709324\n",
      "4.454845588682723\n",
      "4.669745536184336\n",
      "4.44979829552426\n",
      "4.7900797160044615\n",
      "4.433906796992284\n",
      "4.698630148181835\n",
      "4.587782299996932\n",
      "4.379745633250212\n",
      "4.827968169923399\n",
      "4.793323894487464\n",
      "4.562320633918242\n",
      "4.621779593652117\n",
      "0.0\n",
      "7644169\n",
      "[-0.59377638 -0.05881897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:04<02:07, 31.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5853461_annealing\n",
      "[-0.47912372  0.36309777]\n",
      "4.376940413381132\n",
      "4.715534402158328\n",
      "4.800286663175556\n",
      "4.509053058175716\n",
      "4.763156182294777\n",
      "2.747871465646883\n",
      "4.730172678283\n",
      "4.595309164355862\n",
      "4.2805748156632015\n",
      "4.492363269064408\n",
      "4.608499570690703\n",
      "4.652433958805419\n",
      "3.2494497887791924\n",
      "3.7367556320853583\n",
      "4.226742272511043\n",
      "3.2649007988389918\n",
      "4.420878045203521\n",
      "4.34536288040723\n",
      "4.375299156856606\n",
      "4.132552629842279\n",
      "4.442709468620593\n",
      "4.170415795153547\n",
      "4.345159082613538\n",
      "4.568395247561973\n",
      "4.366199785562998\n",
      "4.13319722783903\n",
      "3.204248152873463\n",
      "3.6724664185694262\n",
      "3.417705352735698\n",
      "2.9454748696347446\n",
      "4.610312275457141\n",
      "3.90166721121944\n",
      "4.556064262314514\n",
      "3.9421123039277126\n",
      "4.066036168151728\n",
      "3.5260394011768135\n",
      "4.095464113182144\n",
      "4.28934597350394\n",
      "4.462754320130996\n",
      "3.9496302697563332\n",
      "4.0526143191263575\n",
      "4.48151267819874\n",
      "4.511397356801115\n",
      "3.9785754920982357\n",
      "4.490632016534076\n",
      "4.624256987901298\n",
      "4.366128238595694\n",
      "4.32928392014411\n",
      "4.556755047559272\n",
      "4.200846201777147\n",
      "4.692364674941492\n",
      "4.041113288235322\n",
      "4.545152923953096\n",
      "4.450464320710312\n",
      "4.12426285639073\n",
      "4.388884334871361\n",
      "4.001178086933432\n",
      "4.43573280689665\n",
      "4.466174055555249\n",
      "4.272571092507248\n",
      "-0.0\n",
      "5853461\n",
      "[-0.47912372  0.36309777]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:35<01:35, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6739698_annealing\n",
      "[0.23420799 0.49531111]\n",
      "4.566701922815346\n",
      "4.783072121476106\n",
      "4.444437585029012\n",
      "4.457456860233291\n",
      "4.629511439378403\n",
      "4.592420258387493\n",
      "4.729917216966253\n",
      "4.57639040579575\n",
      "4.233329905375155\n",
      "4.768387392709086\n",
      "4.830682622567415\n",
      "4.50885460022043\n",
      "4.731389677369645\n",
      "4.6629446283790035\n",
      "4.486194683463547\n",
      "4.516535793293285\n",
      "4.77853968750163\n",
      "4.731594069838109\n",
      "4.77648389340549\n",
      "4.304617809386234\n",
      "4.519364643165097\n",
      "4.571024118910629\n",
      "4.493506247467578\n",
      "4.69729123012407\n",
      "4.4993033514140475\n",
      "4.821032269346143\n",
      "4.741116973764361\n",
      "4.675502859559065\n",
      "4.785842288073935\n",
      "4.494466036381909\n",
      "4.8263753434859735\n",
      "4.378099722613835\n",
      "4.132086483272394\n",
      "4.493950005886313\n",
      "4.774450739562414\n",
      "4.39448566120822\n",
      "4.344292067262445\n",
      "4.348408413744621\n",
      "4.537257415476445\n",
      "4.666712338918487\n",
      "4.475556750338679\n",
      "4.684005948147096\n",
      "4.467948613596831\n",
      "4.849554027146621\n",
      "4.573512573918165\n",
      "4.682086529275564\n",
      "4.687678159718625\n",
      "4.51976254705311\n",
      "4.67523993057706\n",
      "4.4364689267318225\n",
      "4.291553091236484\n",
      "4.741749034642646\n",
      "4.790993808399458\n",
      "4.790506362490421\n",
      "4.0793735770550414\n",
      "4.046550626815598\n",
      "4.765273403000515\n",
      "4.748525229819059\n",
      "4.683101835817405\n",
      "4.742934092059527\n",
      "-0.0\n",
      "6739698\n",
      "[0.23420799 0.49531111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:05<01:02, 31.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374564_annealing\n",
      "[-0.00112911  0.52627081]\n",
      "4.664447909753806\n",
      "3.061904324165811\n",
      "4.408994333278449\n",
      "4.088580406244108\n",
      "3.008288202069352\n",
      "3.764061421833256\n",
      "3.369170517753007\n",
      "4.288058640576338\n",
      "4.142967679404823\n",
      "3.9582821458252586\n",
      "3.3925200537619866\n",
      "3.7547356351302588\n",
      "4.608182539171466\n",
      "3.8055988810777848\n",
      "3.7736188555268244\n",
      "4.087618012165982\n",
      "4.5320589126059065\n",
      "3.9696580093977913\n",
      "4.0802692041111985\n",
      "3.667840974299634\n",
      "3.8976235973847904\n",
      "4.043753852412625\n",
      "3.877838596222974\n",
      "4.2116964210131815\n",
      "3.932310625924004\n",
      "2.8651326696324415\n",
      "4.302973817596235\n",
      "3.962073461681759\n",
      "3.8706338568934733\n",
      "4.2848181027017525\n",
      "4.37708620649069\n",
      "4.293487686070354\n",
      "3.9742564211766926\n",
      "4.056725854842205\n",
      "4.515262303915269\n",
      "4.568217777266314\n",
      "4.463109204957785\n",
      "4.280825474210007\n",
      "4.023315922837222\n",
      "4.167806284881343\n",
      "3.447579718326955\n",
      "4.409269203313118\n",
      "3.9594080387474744\n",
      "4.758484690170988\n",
      "4.455604256399133\n",
      "4.682509575345694\n",
      "4.679679406595051\n",
      "4.491068742500783\n",
      "3.998135227415253\n",
      "4.62854745515601\n",
      "4.74584642882871\n",
      "4.176063864945506\n",
      "4.550038781626512\n",
      "4.035962499378955\n",
      "4.506345582416303\n",
      "4.550376123708941\n",
      "4.624111632391285\n",
      "4.623220473757331\n",
      "4.129495612687506\n",
      "4.425480423579085\n",
      "0.0\n",
      "374564\n",
      "[-0.00112911  0.52627081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:35<00:30, 30.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2832983_annealing\n",
      "[-0.53099852  0.20654741]\n",
      "4.645680753198585\n",
      "3.0359656570198417\n",
      "3.026332565544727\n",
      "3.8594511135332823\n",
      "4.1081099530167435\n",
      "4.852966110360101\n",
      "3.1739712541708105\n",
      "4.260960157687952\n",
      "4.63269942521783\n",
      "4.4901398452383\n",
      "3.411373974895944\n",
      "4.524551550203061\n",
      "3.5413487765825966\n",
      "3.581482804145001\n",
      "3.8605604307412014\n",
      "3.532574882645419\n",
      "3.7699098777333893\n",
      "3.749541917461334\n",
      "4.081081035711901\n",
      "3.5942417651560383\n",
      "4.031817948334319\n",
      "3.161238660923241\n",
      "4.249163388866677\n",
      "4.560083475597202\n",
      "4.381437731403147\n",
      "3.541929564537414\n",
      "3.415164258382675\n",
      "4.456005489672065\n",
      "4.040660141793257\n",
      "3.383702590910127\n",
      "3.8056616826494367\n",
      "3.3781859158007306\n",
      "2.7504703391606444\n",
      "4.012596217645437\n",
      "4.323959841294457\n",
      "3.8914853454126965\n",
      "4.117946207082528\n",
      "4.388538045406405\n",
      "4.234366814035374\n",
      "2.9585282484636695\n",
      "2.925625568445262\n",
      "3.7317825753701888\n",
      "4.0131035061726\n",
      "4.540016057545559\n",
      "3.7429428517637438\n",
      "4.149592139218008\n",
      "3.4253558708759364\n",
      "4.560951856695333\n",
      "3.942634403538882\n",
      "4.499954773489043\n",
      "3.4901660599919704\n",
      "3.9298923072249656\n",
      "4.175222559384373\n",
      "4.656225638895895\n",
      "4.328189236123759\n",
      "4.517009264299829\n",
      "4.453692762215209\n",
      "3.5379228928481763\n",
      "4.566715273236345\n",
      "4.481002928109163\n",
      "0.0\n",
      "2832983\n",
      "[-0.53099852  0.20654741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:04<00:00, 30.49s/it]\n"
     ]
    }
   ],
   "source": [
    "### Testing robustness across different initial model configurations and sensorimotor landscapes\n",
    "\n",
    "for i in tqdm(np.arange(n_simulations)):\n",
    "    rseed = seeds[i]\n",
    "\n",
    "    ## Testing a given setting in both annealing and no annealing cases\n",
    "    # parameter ->(seed, annealing  - yes/no, plots - yes/no)\n",
    "    perf_score1 = build_and_simulate(rseed, True, True)                    \n",
    "    perf_score2 = build_and_simulate(rseed, False, True)\n",
    "\n",
    "    ## Terminal performance (mean performance on the last day)\n",
    "    # perf_score1[0] = avg performance on the last day before lesion of BG contribution\n",
    "    # perf_score1[1] = avg performance on the day post lesion of BG contribution\n",
    "    performance_annealing[i] = perf_score1[1]\n",
    "    performance_no_annealing[i] = perf_score2[1]\n",
    "\n",
    "np.save('Results/performance_annealing.npy', performance_annealing)\n",
    "np.save('Results/performance_no_annealing.npy', performance_no_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proportion of simulations that find the global optimum\n",
    "\n",
    "# 0.6 = maximum yield across all local optima\n",
    "# If a simulation has a terminal performance > 0.6 => the simulation must have landed on the global optimum region\n",
    "\n",
    "overall_perf_annealing = np.count_nonzero(performance_annealing>.6)/n_simulations * 100\n",
    "overall_perf_no_annealing = np.count_nonzero(performance_no_annealing>.6)/n_simulations * 100\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of terminal performances with and without annealing\n",
    "\n",
    "plt.plot(performance_annealing, linewidth=0, marker='.', color='g', label='Annealing')\n",
    "plt.plot(performance_no_annealing, linewidth=0, marker='.', color='k', label='No annealing')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.title('Score: ' + str(overall_perf_annealing) + '%,' + str(overall_perf_no_annealing) + '%')\n",
    "\n",
    "plt.savefig('Results/Overall.png')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
